# Self-Organizing-Map-(SOM)
<b> Text Clustering using SOM </b>

Text clustering is an unsupervised process, used to separate a document collection into some 
clusters on the basis of the similarity relationship between documents in the collection. Suppose 
ğ· = {ğ‘‘1, â€¦ , ğ‘‘ğ‘} be a collection of ğ‘ documents to be clustered. The task is to divide ğ· into ğ‘˜
clusters ğ¶1, â€¦ , ğ¶ğ‘˜ where ğ¶1 âˆª â€¦ âˆª ğ¶ğ‘˜ = ğ· and ğ¶ğ‘– âˆ© ğ¶ğ‘— = âˆ…, for ğ‘– â‰  ğ‘—.
<br/>
SOM text clustering can be done in two main phases. The first phase is document preprocessing,
which uses Vector Space Model (VSM) to generate a numeric vector for each text document. In 
the next phase, SOM is applied on the document vectors to obtain document clusters. 
<br/>
<br/>
<b> Dataset </b>
<br/>
In this project, we train and test an SOM network to do cluster analysis of a news 
collection, from the BBC news website corresponding to stories in five topical areas from 2004-
2005. This dataset is a collection of 2225 news document, categorized into 5 classes of 
â€˜businessâ€™, â€˜entertainmentâ€™, â€˜politicsâ€™, â€˜sportâ€™, and â€˜techâ€™.

<b> Phase 1: Document Preprocessing </b>
<br/>
By means of VSM, each document ğ‘‘ğ‘– can be represented by an ğ‘›-dimensional feature vector
ğ’—ğ‘– =< ğ‘£ğ‘–1, â€¦ , ğ‘£ğ‘–ğ‘› >, where ğ‘£ğ‘–ğ‘— is a representation of term ğ‘¡ğ‘—
in document ğ‘‘ğ‘– and ğ‘› is the number 
of distinct terms in the document collection ğ·. 
An approach for computing ğ‘£ğ‘–ğ‘— is the Term Frequency - Inverse Document Frequency (TF-IDF) 
weighting scheme. This method computes ğ‘£ğ‘–ğ‘— for term ğ‘¡ğ‘—
in document ğ‘‘ğ‘– as:
<br/>
![formula](https://user-images.githubusercontent.com/91370511/159134697-02c91891-bf44-47a4-97dc-c3ea5f097b42.PNG)
<br/>

where ğ‘¡ğ‘“ğ‘–ğ‘— is the frequency of term ğ‘¡ğ‘— in document ğ‘‘ğ‘–, and ğ‘‘ğ‘“ğ‘— is the number of documents in ğ· containing term ğ‘¡ğ‘—.
Read â€˜bbc-text.csvâ€™ file and for each document: 
  1. Remove all non-letter characters from the documents.
  2. Extract all words of the document and remove the short words (length â‰¤ 2).
  3. Remove all stop words (e.g., â€˜aâ€™, â€˜andâ€™, â€˜whatâ€™, â€¦), given in file â€˜stopwords.txtâ€™.
  4. Compute the feature vector for each document, using TF-IDF weighting scheme.

<b> Phase 2: SOM Clustering </b>


a) Winner-takes-all approach
  1. Using all documents, build an SOM with one neuron for each class.
  2. Depict the SOM-hits plot.
  3. Compute and report the confusion matrix.

b) On-center, off-surround approach
  1. Using all documents, build an SOM with 3x3 neurons.
  2. Depict the SOM-hits plot.
  3. Compute the Euclidean distance of all documents to their winner neurons and sum up the 
  distances.
  4. Repeat steps 1-3 for 4x4 and 5x5 topologies.
  5. Report and discuss the overall distances of three topologies.
